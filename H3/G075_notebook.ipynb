{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consider the winequality-red.csv dataset (available at the webpage) where the goal is to estimate the quality (sensory appreciation) of a wine based on physicochemical inputs. \n",
    "### Using a 80-20 training-test split with a fixed seed (random_state=0), you are asked to learn MLP regressors to answer the following questions.\n",
    "### Given their stochastic behavior, average the performance of each MLP from 10 runs (for reproducibility consider seeding the MLPs with random_state ∈ {1. .10})."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) [3.5v] Learn a MLP regressor with 2 hidden layers of size 10, rectifier linear unit activation on all nodes, and early stopping with 20% of training data set aside for validation. All remaining parameters (e.g., loss, batch size, regularization term, solver) should be set as default. Plot the distribution of the residues (in absolute value) using a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Reading the CSV file\n",
    "df = pd.read_csv(\"winequality-red.csv\", sep=\";\")\n",
    "\n",
    "X = df.drop(\"quality\", axis=1)  # Drop the \"quality\" column to get the features\n",
    "y = df[\"quality\"]  # Get the \"quality\" column as the target variable\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.values, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Residues\n",
    "res = []\n",
    "\n",
    "for state in range(1, 11):\n",
    "    mlp = MLPRegressor(hidden_layer_sizes=(10, 10), activation='relu', random_state=state, early_stopping=True, validation_fraction=0.2)\n",
    "    mlp.fit(X_train, y_train)\n",
    "\n",
    "    # Predict and calculate the residues\n",
    "    pred = mlp.predict(X_test)\n",
    "    residue = np.abs(y_test - pred)\n",
    "    res.extend(residue)\n",
    "\n",
    "# Plot the histogram of the residues using Seaborn\n",
    "sns.histplot(res, bins = 30)\n",
    "plt.title(\"Distribution of Residues (Absolute Value)\")\n",
    "plt.xlabel(\"Residues (Absolute Value)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) [1.5v] Since we are in the presence of a integer regression task, a recommended trick is to round and bound estimates. Assess the impact of these operations on the MAE of the MLP learnt in previous question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Initialize lists to store MAE before and after rounding and bounding\n",
    "mae_original = []\n",
    "mae_round = []\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.values, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Loop through random seeds from 1 to 10\n",
    "for state in range(1, 11):\n",
    "    \n",
    "    mlp = MLPRegressor(hidden_layer_sizes=(10,10), activation = 'relu', random_state=state,\n",
    "                   early_stopping = True, validation_fraction = 0.2)\n",
    "    mlp.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = mlp.predict(X_test)\n",
    "\n",
    "    # Calculate the MAE before rounding and bounding\n",
    "    mae_original.append(mean_absolute_error(y_test, y_pred))\n",
    "    \n",
    "    \n",
    "    # Round and Bound estimates\n",
    "    round_pred = np.round(y_pred) \n",
    "    y_rounded = np.clip(round_pred, 1, 10)  \n",
    "    \n",
    "    # Calculate the MAE after rounding and bounding\n",
    "    mae_round.append(mean_absolute_error(y_test, y_rounded))\n",
    "\n",
    "# Calculate the average MAE before and after rounding and bounding\n",
    "average_mae_original = np.mean(mae_original)\n",
    "average_mae_round = np.mean(mae_round)\n",
    "\n",
    "# Print the average MAE before and after\n",
    "print(f\"\\nAverage MAE Before Round and Bound Estimates = {average_mae_original}\")\n",
    "print(f\"Average MAE After Round and Bound Estimates = {average_mae_round}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) [1.5v] Similarly assess the impact on RMSE from replacing early stopping by a well-defined number of iterations in {20,50,100,200} (where one iteration corresponds to a batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# MLP with Early Stopping\n",
    "rmse_original = []  \n",
    "\n",
    "# Arrays for each max_iterations\n",
    "rmse_20 = []\n",
    "rmse_50 = []\n",
    "rmse_100 = []\n",
    "rmse_200 = []\n",
    "\n",
    "rmse_arrays = [rmse_20, rmse_50, rmse_100, rmse_200]\n",
    "\n",
    "\n",
    "iterations = [20, 50, 100, 200]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.values, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Early stopping MLP for RMSE comparison in exercise 4\n",
    "for random_state in range(1, 11):\n",
    "    \n",
    "    original_mlp = MLPRegressor(hidden_layer_sizes=(10,10), activation='relu', random_state=random_state,\n",
    "                                early_stopping=True, validation_fraction=0.2)\n",
    "    original_mlp.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_original = original_mlp.predict(X_test)\n",
    "\n",
    "    # RMSE\n",
    "    rmse_original.append(sqrt(mean_squared_error(y_test, y_pred_original)))\n",
    "\n",
    "\n",
    "# Max iterations \n",
    "for num_iterations in iterations:\n",
    "    for random_state in range(1, 11):\n",
    "\n",
    "        mlp_iterations = MLPRegressor(hidden_layer_sizes=(10,10), activation='relu', \n",
    "                                      random_state=random_state, max_iter=num_iterations)\n",
    "        mlp_iterations.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "        y_pred_iterations = mlp_iterations.predict(X_test)\n",
    "\n",
    "        # Calcula rmse\n",
    "        rmse = sqrt(mean_squared_error(y_test, y_pred_iterations))\n",
    "        \n",
    "        if num_iterations == 20:\n",
    "            rmse_20.append(rmse)\n",
    "        elif num_iterations == 50:\n",
    "            rmse_50.append(rmse)\n",
    "        elif num_iterations == 100:\n",
    "            rmse_100.append(rmse)\n",
    "        elif num_iterations == 200:\n",
    "            rmse_200.append(rmse)\n",
    "\n",
    "means = []\n",
    "\n",
    "# For better code, iterates through the array containing all the iterations\n",
    "for i, rmse_array in enumerate(rmse_arrays):\n",
    "    mean_value = np.mean(rmse_array)\n",
    "    means.append((iterations[i], mean_value))\n",
    "\n",
    "# Prints\n",
    "for mean_rmse in means:\n",
    "    print(f\"Mean RMSE of {mean_rmse[0]} iterations: {mean_rmse[1]}\")\n",
    "\n",
    "\n",
    "mean_rmse_original = np.mean(rmse_original)\n",
    "\n",
    "print(f\"\\nMean RMSE with Early Stopping = {mean_rmse_original}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) [1.5v] Critically comment the results obtained in previous question, hypothesizing at least one reason why early stopping favors and/or worsens performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterações do modelo com early stopping\n",
    "# Usado para comparar os modelos\n",
    "print(original_mlp.n_iter_) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stopping para o treino do modelo quando a performance em relação aos testes de validação começa a descer. Deste modo, este método é bom para evitar tanto o underfitting como o overfitting, treinando até ao momento que a accuracy para novos dados começa a descer. Com o número máximo de iterações podemos notar que o RMSE é maior para todos os modelos, exceto o que tem um máximo de 200 iterações. Podemos então concluir que nos outros modelos (20, 50 e 100 iterações) houve underfitting, não tendo iterações suficientes para treinar o modelo adequadamente (o que não acontece com o early stopping). \n",
    "\n",
    "O early stopping separa os dados de treino em validation e training sets. Uma das suas desvantagens é o facto de ser sensível à escolha do validation set: caso o validation set seja muito pequeno ou não muito representativo do testing set, o modelo pode parar muito cedo ou muito tarde, levando a underfitting ou overfitting. Uma vez que os modelos com early stopping e max_iterations a 200 param nas 200 iterações (no caso do early stopping, porque é o max_iterations por default), e sendo o RMSE do modelo com early stopping maior, concluímos que o validation set não deve ser o mais representativo do testing set, levando a um erro maior."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
