{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consider the winequality-red.csv dataset (available at the webpage) where the goal is to estimate the quality (sensory appreciation) of a wine based on physicochemical inputs. \n",
    "### Using a 80-20 training-test split with a fixed seed (random_state=0), you are asked to learn MLP regressors to answer the following questions.\n",
    "### Given their stochastic behavior, average the performance of each MLP from 10 runs (for reproducibility consider seeding the MLPs with random_state ∈ {1. .10})."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) [3.5v] Learn a MLP regressor with 2 hidden layers of size 10, rectifier linear unit activation on all nodes, and early stopping with 20% of training data set aside for validation. All remaining parameters (e.g., loss, batch size, regularization term, solver) should be set as default. Plot the distribution of the residues (in absolute value) using a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## APAGAR ################################################################################################\n",
    "################################################################################################\n",
    "################################################################################################\n",
    "################################################################################################\n",
    "################################################################################################\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "# Reading the CSV file\n",
    "df = pd.read_csv(\"winequality-red.csv\", delimiter=\";\")\n",
    "\n",
    "X = df.drop(\"quality\", axis=1)  # Drop the \"quality\" column to get the features\n",
    "y = df[\"quality\"]  # Get the \"quality\" column as the target variable\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "# Residues \n",
    "res = []\n",
    "\n",
    "for state in range(1, 11):\n",
    "    mlp = MLPRegressor(hidden_layer_sizes=(10,10), activation = 'relu', random_state=state,\\\n",
    "                   early_stopping = True, validation_fraction = 0.2)\n",
    "    mlp.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict and calculate the residues\n",
    "    pred = mlp.predict(X_test)\n",
    "\n",
    "    res.append(np.abs(y_test - pred))\n",
    "    \n",
    "plt.hist(res, bins=20, alpha=0.5, label=[f'Seed {i}' for i in range(1, 11)])\n",
    "plt.xlabel('Residues')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Residues')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "##\n",
    "## Possiveis alteracoes:\n",
    "##  1- Make regression anted do X_train (como usado na doscumentação)\n",
    "##  2- Max_iters dentro do MLP_regressor (retirei pois não me lembro porque o pus lá)\n",
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) [1.5v] Since we are in the presence of a integer regression task, a recommended trick is to round and bound estimates. Assess the impact of these operations on the MAE of the MLP learnt in previous question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Initialize lists to store MAE before and after rounding and bounding\n",
    "mae_original = []\n",
    "mae_round = []\n",
    "\n",
    "# Define lower and upper bounds for estimates\n",
    "lower_bound = 0  # Adjust as needed\n",
    "upper_bound = 10  # Adjust as needed\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Loop through random seeds from 1 to 10\n",
    "for state in range(1, 11):\n",
    "    # Create and train the MLP regressor\n",
    "    mlp = MLPRegressor(hidden_layer_sizes=(10,10), activation = 'relu', random_state=state,\\\n",
    "                   early_stopping = True, validation_fraction = 0.2)\n",
    "    mlp.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict and calculate the residues\n",
    "    pred = mlp.predict(X_test)\n",
    "\n",
    "    # Calculate the MAE before rounding and bounding\n",
    "    mae_original.append(mean_absolute_error(y_test, pred))\n",
    "    \n",
    "    rounded_predictions = np.round(pred)  # Round to the nearest integer\n",
    "    #y_rounded = np.clip(rounded_predictions, lower_bound, upper_bound)  # Bound the estimates within a range\n",
    "    #mae_round.append(mean_absolute_error(y_test, y_rounded))\n",
    "    \n",
    "    # Calculate the MAE after rounding and bounding\n",
    "    mae_round.append(mean_absolute_error(y_test, rounded_predictions))\n",
    "\n",
    "# Print the MAE before and after rounding and bounding for each run\n",
    "for i in range(10):\n",
    "    print(f\"State {i+1}: MAE Before = {mae_original[i]:.4f}, MAE After = {mae_round[i]:.4f}\")\n",
    "\n",
    "# Calculate the average MAE before and after rounding and bounding\n",
    "average_mae_original = np.mean(mae_original)\n",
    "average_mae_round = np.mean(mae_round)\n",
    "\n",
    "# Print the average MAE before and after\n",
    "print(f\"Average MAE Before Round and Bound Estiamtes = {average_mae_original:.4f}\")\n",
    "print(f\"Average MAE After Round and Bound Estimates = {average_mae_round:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) [1.5v] Similarly assess the impact on RMSE from replacing early stopping by a well-defined number of iterations in {20,50,100,200} (where one iteration corresponds to a batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# Initialize lists to store RMSE for different numbers of iterations\n",
    "rmse_original = []  # RMSE for MLP with early stopping\n",
    "rmse_iterations = []  # RMSE for MLP with a fixed number of iterations\n",
    "\n",
    "# Define the number of iterations to assess\n",
    "iterations_to_assess = [20, 50, 100, 200]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Loop through random seeds from 1 to 10\n",
    "for random_state in range(1, 11):\n",
    "    # Split your data into training and testing sets (80-20 split)\n",
    "\n",
    "    # Create and train the original MLP regressor with early stopping\n",
    "    original_mlp = MLPRegressor(hidden_layer_sizes=(10,10), activation='relu', early_stopping=True,\\\n",
    "                                validation_fraction=0.2, random_state=random_state)\n",
    "    original_mlp.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions with the original MLP\n",
    "    y_pred_original = original_mlp.predict(X_test)\n",
    "\n",
    "    # Calculate the RMSE for the original MLP\n",
    "    rmse_original.append(sqrt(mean_squared_error(y_test, y_pred_original)))\n",
    "\n",
    "    # Compare with different numbers of iterations\n",
    "    for num_iterations in iterations_to_assess:\n",
    "        # Create and train the MLP regressor with a fixed number of iterations\n",
    "        mlp_iterations = MLPRegressor(hidden_layer_sizes=(10,10), activation='relu', \\\n",
    "                                      max_iter=num_iterations, random_state=random_state)\n",
    "        mlp_iterations.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions on the test data\n",
    "        y_pred_iterations = mlp_iterations.predict(X_test)\n",
    "\n",
    "        # Calculate the RMSE for the current number of iterations\n",
    "        rmse = sqrt(mean_squared_error(y_test, y_pred_iterations))\n",
    "        rmse_iterations.append((num_iterations, rmse))\n",
    "\n",
    "# Print and compare the RMSE for the original MLP and different numbers of iterations\n",
    "for num_iterations, rmse in rmse_iterations:\n",
    "    print(f\"Iterations={num_iterations}: RMSE={rmse:.4f}\")\n",
    "\n",
    "average_rmse_original = np.mean(rmse_original)\n",
    "print(f\"Average RMSE with Early Stopping = {average_rmse_original:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) [1.5v] Critically comment the results obtained in previous question, hypothesizing at least one reason why early stopping favors and/or worsens performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
