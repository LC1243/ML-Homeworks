{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consider the winequality-red.csv dataset (available at the webpage) where the goal is to estimate the quality (sensory appreciation) of a wine based on physicochemical inputs. \n",
    "### Using a 80-20 training-test split with a fixed seed (random_state=0), you are asked to learn MLP regressors to answer the following questions.\n",
    "### Given their stochastic behavior, average the performance of each MLP from 10 runs (for reproducibility consider seeding the MLPs with random_state ∈ {1. .10})."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) [3.5v] Learn a MLP regressor with 2 hidden layers of size 10, rectifier linear unit activation on all nodes, and early stopping with 20% of training data set aside for validation. All remaining parameters (e.g., loss, batch size, regularization term, solver) should be set as default. Plot the distribution of the residues (in absolute value) using a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "    \n",
    "# Reading the CSV file\n",
    "df = pd.read_csv(\"winequality-red.csv\", delimiter=\";\")\n",
    "\n",
    "X = df.drop(\"quality\", axis=1)  # Drop the \"quality\" column to get the features\n",
    "y = df[\"quality\"]  # Get the \"quality\" column as the target variable\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "# Residues \n",
    "res = []\n",
    "\n",
    "for state in range(1, 11):\n",
    "    mlp = MLPRegressor(hidden_layer_sizes=(10,10), activation = 'relu', random_state=state,\\\n",
    "                   early_stopping = True, validation_fraction = 0.2)\n",
    "    mlp.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict and calculate the residues\n",
    "    pred = mlp.predict(X_test)\n",
    "\n",
    "    res.append(np.abs(y_test - pred))\n",
    "    \n",
    "plt.hist(res, bins=20, alpha=0.5, label=[f'Seed {i}' for i in range(1, 11)])\n",
    "plt.xlabel('Residues')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Residues')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "##\n",
    "## Possiveis alteracoes:\n",
    "##  1- Make regression anted do X_train (como usado na doscumentação)\n",
    "##  2- Max_iters dentro do MLP_regressor (retirei pois não me lembro porque o pus lá)\n",
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) [1.5v] Since we are in the presence of a integer regression task, a recommended trick is to round and bound estimates. Assess the impact of these operations on the MAE of the MLP learnt in previous question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Initialize lists to store MAE before and after rounding and bounding\n",
    "mae_original = []\n",
    "mae_round = []\n",
    "\n",
    "# Define lower and upper bounds for estimates\n",
    "lower_bound = 0  # Adjust as needed\n",
    "upper_bound = 10  # Adjust as needed\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Loop through random seeds from 1 to 10\n",
    "for state in range(1, 11):\n",
    "    # Create and train the MLP regressor\n",
    "    mlp = MLPRegressor(hidden_layer_sizes=(10,10), activation = 'relu', random_state=state,\\\n",
    "                   early_stopping = True, validation_fraction = 0.2)\n",
    "    mlp.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict and calculate the residues\n",
    "    pred = mlp.predict(X_test)\n",
    "\n",
    "    # Calculate the MAE before rounding and bounding\n",
    "    mae_original.append(mean_absolute_error(y_test, pred))\n",
    "    \n",
    "    rounded_predictions = np.round(pred)  # Round to the nearest integer\n",
    "    #y_rounded = np.clip(rounded_predictions, lower_bound, upper_bound)  # Bound the estimates within a range\n",
    "    #mae_round.append(mean_absolute_error(y_test, y_rounded))\n",
    "    \n",
    "    # Calculate the MAE after rounding and bounding\n",
    "    mae_round.append(mean_absolute_error(y_test, rounded_predictions))\n",
    "\n",
    "# Print the MAE before and after rounding and bounding for each run\n",
    "for i in range(10):\n",
    "    print(f\"State {i+1}: MAE Before = {mae_original[i]}, MAE After = {mae_round[i]}\")\n",
    "\n",
    "# Calculate the average MAE before and after rounding and bounding\n",
    "average_mae_original = np.mean(mae_original)\n",
    "average_mae_round = np.mean(mae_round)\n",
    "\n",
    "# Print the average MAE before and after\n",
    "print(f\"\\nAverage MAE Before Round and Bound Estiamtes = {average_mae_original}\")\n",
    "print(f\"Average MAE After Round and Bound Estimates = {average_mae_round}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) [1.5v] Similarly assess the impact on RMSE from replacing early stopping by a well-defined number of iterations in {20,50,100,200} (where one iteration corresponds to a batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# MLP com early stopping\n",
    "rmse_original = []  \n",
    "\n",
    "# Arrays para cada max_iterations\n",
    "rmse_20 = []\n",
    "rmse_50 = []\n",
    "rmse_100 = []\n",
    "rmse_200 = []\n",
    "\n",
    "rmse_arrays = [rmse_20, rmse_50, rmse_100, rmse_200]\n",
    "\n",
    "\n",
    "iterations = [20, 50, 100, 200]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Early stopping MLP\n",
    "for random_state in range(1, 11):\n",
    "\n",
    "    original_mlp = MLPRegressor(hidden_layer_sizes=(10,10), activation='relu', early_stopping=True,\\\n",
    "                                validation_fraction=0.2, random_state=random_state)\n",
    "    original_mlp.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_original = original_mlp.predict(X_test)\n",
    "\n",
    "    # RMSE\n",
    "    rmse_original.append(sqrt(mean_squared_error(y_test, y_pred_original)))\n",
    "\n",
    "    \n",
    "# Max iterations \n",
    "for num_iterations in iterations:\n",
    "    for random_state in range(1, 11):\n",
    "\n",
    "        mlp_iterations = MLPRegressor(hidden_layer_sizes=(10,10), activation='relu', \\\n",
    "                                    max_iter=num_iterations, random_state=random_state)\n",
    "        mlp_iterations.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "        y_pred_iterations = mlp_iterations.predict(X_test)\n",
    "\n",
    "        # Calcula rmse\n",
    "        rmse = sqrt(mean_squared_error(y_test, y_pred_iterations))\n",
    "        \n",
    "        if num_iterations == 20:\n",
    "            rmse_20.append(rmse)\n",
    "        elif num_iterations == 50:\n",
    "            rmse_50.append(rmse)\n",
    "        elif num_iterations == 100:\n",
    "            rmse_100.append(rmse)\n",
    "        elif num_iterations == 200:\n",
    "            rmse_200.append(rmse)\n",
    "\n",
    "means = []\n",
    "\n",
    "# Para melhor codigo, itera pelo array que contem todas as iteracoes\n",
    "for i, rmse_array in enumerate(rmse_arrays):\n",
    "    mean_value = np.mean(rmse_array)\n",
    "    means.append((iterations[i], mean_value))\n",
    "\n",
    "# Prints\n",
    "for mean_rmse in means:\n",
    "    print(f\"Mean RMSE of {mean_rmse[0]} iterations: {mean_rmse[1]}\")\n",
    "\n",
    "\n",
    "mean_rmse_original = np.mean(rmse_original)\n",
    "\n",
    "print(f\"\\nMean RMSE with Early Stopping = {mean_rmse_original}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) [1.5v] Critically comment the results obtained in previous question, hypothesizing at least one reason why early stopping favors and/or worsens performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
