{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63d3e5b7",
   "metadata": {},
   "source": [
    "# I. Pen-and-paper [11v]\n",
    "#### version 2 changes in blue (21/10/23)\n",
    "### Given the following observations, {(1 0.6 0.1) , (0 âˆ’0.4 0.8) , (0 0.2 0.5) , (1 0.4 âˆ’0.1)}. Consider a Bayesian clustering that assumes independence {ğ‘¦1} and {ğ‘¦2, ğ‘¦3}, two clusters following a Bernoulli distribution on ğ‘¦1 (ğ‘1 and ğ‘2), a multivariate Gaussian on {ğ‘¦2, ğ‘¦3} (ğ‘1 and ğ‘2), and the following initial mixture:\n",
    "### <center> ğœ‹1 = 0.5, ğœ‹2 = 0.5 </center>\n",
    "### <center> ğ‘1 = ğ‘ƒ(ğ‘¦1 = 1) = 0.3,  ğ‘2 = ğ‘ƒ(ğ‘¦1 = 1) = 0.7  </center>\n",
    "### <center> ğ‘1 (ğ®1 = (1 1) , ğšº1 = (2 0.5, 0.5 2)), ğ‘2 (ğ®2 = (0 0) , ğšº2 = (1.5 1, 1 1.5)).  </center>\n",
    "### Note: you can solve this exercise by neglecting ğ‘¦1 and still scoring up to 70% of its grade."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c04cec",
   "metadata": {},
   "source": [
    "### 1) [6v] Perform one epoch of the EM clustering algorithm and determine the new parameters. <br>Hint: we suggest you to use numpy and scipy, however disclose the intermediary results step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f47bb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# priors\n",
    "pi = np.array([0.5, 0.5])\n",
    "\n",
    "# Cluster 1 (N1)\n",
    "mu1 = np.array([1, 1])\n",
    "cov1 = np.array([[2, 0.5], [0.5, 2]])\n",
    "\n",
    "# Cluster 2 (N2)\n",
    "mu2 = np.array([0, 0])\n",
    "cov2 = np.array([[1.5, 1], [1, 1.5]])\n",
    "\n",
    "p1 = 0.3\n",
    "p2 = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4025f044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian 1, observation  1 : 0.06657529920303393\n",
      "Gaussian 2, observation  1 : 0.11961837142058572\n",
      "probs before normalization [0.009986294880455089, 0.041866429997205]\n",
      "[0.19258959 0.80741041]\n",
      "Gaussian 1, observation  2 : 0.05004888824270901\n",
      "Gaussian 2, observation  2 : 0.0681905803254947\n",
      "probs before normalization [0.017517110884948152, 0.010228587048824206]\n",
      "[0.63134512 0.36865488]\n",
      "Gaussian 1, observation  3 : 0.06837452355368487\n",
      "Gaussian 2, observation  3 : 0.12958103481626038\n",
      "probs before normalization [0.023931083243789703, 0.019437155222439058]\n",
      "[0.55181128 0.44818872]\n",
      "Gaussian 1, observation  4 : 0.059046993443730274\n",
      "Gaussian 2, observation  4 : 0.12450008976589248\n",
      "probs before normalization [0.00885704901655954, 0.04357503141806237]\n",
      "[0.16892423 0.83107577]\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "\n",
    "# Given observations\n",
    "observations = np.array([\n",
    "    [1, 0.6, 0.1],\n",
    "    [0, -0.4, 0.8],\n",
    "    [0, 0.2, 0.5],\n",
    "    [1, 0.4, -0.1]\n",
    "])\n",
    "observations_y2_y3 = observations[:, 1:]  # Considering only y2 and y3\n",
    "\n",
    "# Number of observations\n",
    "num_observations = len(observations)\n",
    "\n",
    "# Initializing responsibilities\n",
    "responsibilities = np.zeros((num_observations, 2))\n",
    "\n",
    "for i in range(num_observations):\n",
    "    # Calculate the probability of each observation (y2, y3) belonging to each cluster\n",
    "    prob_cluster1 = pi[0] * multivariate_normal.pdf(observations_y2_y3[i], mean=mu1, cov=cov1)\n",
    "    print(\"Gaussian 1, observation \", i+1, \":\", multivariate_normal.pdf(observations_y2_y3[i], mean=mu1, cov=cov1))\n",
    "    prob_cluster2 = pi[1] * multivariate_normal.pdf(observations_y2_y3[i], mean=mu2, cov=cov2)\n",
    "    print(\"Gaussian 2, observation \", i+1, \":\", multivariate_normal.pdf(observations_y2_y3[i], mean=mu2, cov=cov2))\n",
    "    \n",
    "    # Multiply by y1 probabilities for each cluster\n",
    "    prob_cluster1 *= p1 if observations[i, 0] == 1 else (1 - p1)\n",
    "    prob_cluster2 *= p2 if observations[i, 0] == 1 else (1 - p2)\n",
    "    \n",
    "    print(\"probs before normalization\", [prob_cluster1, prob_cluster2])\n",
    "    # Normalize the responsibilities\n",
    "    responsibilities[i] = [prob_cluster1 / (prob_cluster1 + prob_cluster2),\n",
    "                           prob_cluster2 / (prob_cluster1 + prob_cluster2)]\n",
    "    \n",
    "    print(responsibilities[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39f01e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the parameters\n",
    "# Calculate new pi (cluster weights)\n",
    "new_pi = np.mean(responsibilities, axis=0)\n",
    "\n",
    "# Calculate new means for clusters (considering y2 and y3 only)\n",
    "new_mu1 = np.dot(responsibilities[:, 0], observations[:, 1:]) / np.sum(responsibilities[:, 0])\n",
    "new_mu2 = np.dot(responsibilities[:, 1], observations[:, 1:]) / np.sum(responsibilities[:, 1])\n",
    "\n",
    "# Calculate new covariance matrices for clusters (considering y2 and y3 only)\n",
    "diff1 = observations[:, 1:] - new_mu1\n",
    "cov1 = np.dot(responsibilities[:, 0] * diff1.T, diff1) / np.sum(responsibilities[:, 0])\n",
    "\n",
    "diff2 = observations[:, 1:] - new_mu2\n",
    "cov2 = np.dot(responsibilities[:, 1] * diff2.T, diff2) / np.sum(responsibilities[:, 1])\n",
    "\n",
    "# Update the Bernoulli probabilities\n",
    "p1 = np.sum(responsibilities[:, 0] * observations[:, 0]) / np.sum(responsibilities[:, 0])\n",
    "p2 = np.sum(responsibilities[:, 1] * observations[:, 0]) / np.sum(responsibilities[:, 1])\n",
    "\n",
    "# Update the parameters\n",
    "pi = new_pi\n",
    "mu1 = new_mu1\n",
    "mu2 = new_mu2\n",
    "cov1 = cov1\n",
    "cov2 = cov2\n",
    "p1 = p1\n",
    "p2 = p2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c1d62ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Priors: [0.38616755 0.61383245]\n",
      "Means u1: [0.026509   0.50712978]\n",
      "Sigma 1: [[ 0.14136501 -0.10540546]\n",
      " [-0.10540546  0.0960526 ]]\n",
      "Means u2: [0.30914476 0.2104205 ]\n",
      "Sigma 2: [[ 0.10829305 -0.08865175]\n",
      " [-0.08865175  0.1041233 ]]\n",
      "New P1: 0.23403948408541084\n",
      "New P2: 0.6673181710330361\n"
     ]
    }
   ],
   "source": [
    "print(\"Priors:\", pi)\n",
    "print(\"Means u1:\", mu1)\n",
    "print(\"Sigma 1:\", cov1)\n",
    "print(\"Means u2:\", mu2)\n",
    "print(\"Sigma 2:\", cov2)\n",
    "print(\"New P1:\", p1)\n",
    "print(\"New P2:\", p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b4f2af",
   "metadata": {},
   "source": [
    "### 2) [2v] Given the new observation, Xğ‘›ğ‘’ğ‘¤ = (1 0.3 0.7), determine the cluster memberships (posteriors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e60f45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian 1: 0.027075573673303183\n",
      "Gaussian 2: 0.06843088109574726\n",
      "Posterior 1 before normalization: 0.002447048524076359\n",
      "Posterior 2 before normalization: 0.028030763221841403\n",
      "Posterior for Cluster 1: 0.08028950846197545\n",
      "Posterior for Cluster 2: 0.9197104915380245\n"
     ]
    }
   ],
   "source": [
    "# New observation\n",
    "x_new = np.array([1, 0.3, 0.7])\n",
    "\n",
    "# Calculate probabilities of the new observation (y2, y3) belonging to each cluster\n",
    "prob_cluster1 = pi[0] * multivariate_normal.pdf(x_new[1:], mean=mu1, cov=cov1)\n",
    "print(\"Gaussian 1:\", multivariate_normal.pdf(x_new[1:], mean=mu1, cov=cov1))\n",
    "prob_cluster2 = pi[1] * multivariate_normal.pdf(x_new[1:], mean=mu2, cov=cov2)\n",
    "print(\"Gaussian 2:\", multivariate_normal.pdf(x_new[1:], mean=mu2, cov=cov2))\n",
    "\n",
    "# Multiply by y1 probabilities for each cluster\n",
    "prob_cluster1 *= p1 if x_new[0] == 1 else (1 - p1)\n",
    "prob_cluster2 *= p2 if x_new[0] == 1 else (1 - p2)\n",
    "\n",
    "print(\"Posterior 1 before normalization:\", prob_cluster1)\n",
    "print(\"Posterior 2 before normalization:\", prob_cluster2)\n",
    "# Calculate the posteriors (responsibilities) for the new observation\n",
    "posterior_cluster1 = prob_cluster1 / (prob_cluster1 + prob_cluster2)\n",
    "posterior_cluster2 = prob_cluster2 / (prob_cluster1 + prob_cluster2)\n",
    "\n",
    "print(\"Posterior for Cluster 1:\", posterior_cluster1)\n",
    "print(\"Posterior for Cluster 2:\", posterior_cluster2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9405586",
   "metadata": {},
   "source": [
    "### 3) [2.5v] Performing a hard assignment of observations to clusters under a ML assumption, identify the silhouette of the larger cluster under a Manhattan distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3835c4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs under ML assumption [0.2314743376774436, 0.9495425230090515]\n",
      "Observation 1 assigned to Cluster 2\n",
      "\n",
      "probs under ML assumption [1.266332483325112, 0.08873672123767436]\n",
      "Observation 2 assigned to Cluster 1\n",
      "\n",
      "probs under ML assumption [1.438110403645913, 0.45417449728697806]\n",
      "Observation 3 assigned to Cluster 1\n",
      "\n",
      "probs under ML assumption [0.02076522613490563, 0.7233119821229737]\n",
      "Observation 4 assigned to Cluster 2\n",
      "\n",
      "Assigned Clusters: [1, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "assigned_clusters = []  # List to hold assigned clusters\n",
    "\n",
    "for i in range(num_observations):\n",
    "    # Calculate the probability of each observation (y2, y3) belonging to each cluster\n",
    "    prob_cluster1 = multivariate_normal.pdf(observations_y2_y3[i], mean=mu1, cov=cov1)\n",
    "    #print(\"Gaussian 1, observation \", i+1, \":\", multivariate_normal.pdf(observations_y2_y3[i], mean=mu1, cov=cov1))\n",
    "    prob_cluster2 = multivariate_normal.pdf(observations_y2_y3[i], mean=mu2, cov=cov2)\n",
    "    #print(\"Gaussian 2, observation \", i+1, \":\", multivariate_normal.pdf(observations_y2_y3[i], mean=mu2, cov=cov2))\n",
    "    \n",
    "    # Multiply by y1 probabilities for each cluster\n",
    "    prob_cluster1 *= p1 if observations[i, 0] == 1 else (1 - p1)\n",
    "    prob_cluster2 *= p2 if observations[i, 0] == 1 else (1 - p2)\n",
    "    \n",
    "    print(\"probs under ML assumption\", [prob_cluster1, prob_cluster2])    \n",
    "    \n",
    "    # Decide the assigned cluster for the observation based on probabilities\n",
    "    assigned_cluster = 1 if prob_cluster2 > prob_cluster1 else 0\n",
    "    assigned_clusters.append(assigned_cluster)\n",
    "    print(f\"Observation {i+1} assigned to Cluster {assigned_cluster + 1}\\n\")\n",
    "\n",
    "# Print assigned clusters\n",
    "print(\"Assigned Clusters:\", assigned_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5a50f00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation 1: Silhouette Score -> 0.8222222222222223\n",
      "Observation 2: Silhouette Score -> 0.6666666666666666\n",
      "Observation 3: Silhouette Score -> 0.4999999999999999\n",
      "Observation 4: Silhouette Score -> 0.8222222222222223\n",
      "Cluster C1 Silhouette Score: 0.5833333333333333\n",
      "Cluster C2 Silhouette Score: 0.8222222222222223\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "# Assuming 'assigned_clusters' and 'observations' are defined in your code\n",
    "\n",
    "# Calculate pairwise Manhattan distances between observations\n",
    "manhattan_distances = pairwise_distances(observations, metric='manhattan')\n",
    "\n",
    "silhouette_scores = []\n",
    "\n",
    "for i, observation in enumerate(observations):\n",
    "    cluster_index = assigned_clusters[i]\n",
    "    if cluster_index == 1:\n",
    "        other_cluster_index = 0  # Index of the other cluster\n",
    "    else:\n",
    "        other_cluster_index = 1\n",
    "\n",
    "    # Calculate the average distance to other points in the same cluster\n",
    "    same_cluster_mask = np.array(assigned_clusters) == cluster_index\n",
    "    same_cluster_mask[i] = False  # Exclude the current index 'i'\n",
    "    same_cluster_distances = manhattan_distances[i][same_cluster_mask]\n",
    "    a = np.mean(same_cluster_distances)\n",
    "\n",
    "    # Calculate the average distance to points in the nearest cluster\n",
    "    nearest_cluster_mask = np.array(assigned_clusters) == other_cluster_index\n",
    "    nearest_cluster_mask[i] = False  # Exclude the current index 'i'\n",
    "    nearest_cluster_distances = manhattan_distances[i][nearest_cluster_mask]\n",
    "    b = np.mean(nearest_cluster_distances)\n",
    "\n",
    "    # Calculate silhouette score for the observation\n",
    "    silhouette = (b - a) / np.maximum(a, b)\n",
    "    silhouette_scores.append(silhouette)\n",
    "\n",
    "# Print silhouette scores for each observation\n",
    "for i, score in enumerate(silhouette_scores):\n",
    "    print(f\"Observation {i + 1}: Silhouette Score -> {score}\")\n",
    "\n",
    "silhouette_cluster_1 = [silhouette_scores[i] for i in range(len(assigned_clusters)) if assigned_clusters[i] == 0]\n",
    "print(\"Cluster C1 Silhouette Score:\", sum(silhouette_cluster_1)/len(silhouette_cluster_1) )\n",
    "silhouette_cluster_2 = [silhouette_scores[i] for i in range(len(assigned_clusters)) if assigned_clusters[i] == 1]\n",
    "print(\"Cluster C2 Silhouette Score:\", sum(silhouette_cluster_2)/len(silhouette_cluster_2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e324c111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation 1: Silhouette Score -> 0.8222222222222223\n",
      "Observation 2: Silhouette Score -> 0.6666666666666666\n",
      "Observation 3: Silhouette Score -> 0.4999999999999999\n",
      "Observation 4: Silhouette Score -> 0.8222222222222223\n",
      "Cluster C1 Silhouette Score: 0.5833333333333333\n",
      "Cluster C2 Silhouette Score: 0.8222222222222223\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "silhouette_scores = []\n",
    "\n",
    "for i, observation in enumerate(observations):\n",
    "    cluster_index = assigned_clusters[i]\n",
    "    if cluster_index == 1:\n",
    "        other_cluster_index = 0  # Assign the opposite cluster index\n",
    "    else:\n",
    "        other_cluster_index = 1\n",
    "\n",
    "    # Calculate the average distance to other points in the same cluster using the a_fn function\n",
    "    same_cluster_distances = [\n",
    "        distance.cityblock(observation, obs) for obs, idx in zip(observations, assigned_clusters) if idx == cluster_index and not np.array_equal(observation, obs)\n",
    "    ]\n",
    "    a = sum(same_cluster_distances) / len(same_cluster_distances)\n",
    "\n",
    "    # Calculate the average distance to points in the nearest cluster using the a_fn function\n",
    "    nearest_cluster_distances = [\n",
    "        distance.cityblock(observation, obs) for obs, idx in zip(observations, assigned_clusters) if idx == other_cluster_index\n",
    "    ]\n",
    "    b = sum(nearest_cluster_distances) / len(nearest_cluster_distances)\n",
    "\n",
    "    # Calculate silhouette score for the observation\n",
    "    silhouette = (b - a) / max(a, b)\n",
    "    silhouette_scores.append(silhouette)\n",
    "\n",
    "# Print silhouette scores for each observation\n",
    "for i, score in enumerate(silhouette_scores):\n",
    "    print(f\"Observation {i + 1}: Silhouette Score -> {score}\")\n",
    "\n",
    "silhouette_cluster_1 = [silhouette_scores[i] for i in range(len(assigned_clusters)) if assigned_clusters[i] == 0]\n",
    "print(\"Cluster C1 Silhouette Score:\", sum(silhouette_cluster_1)/len(silhouette_cluster_1) )\n",
    "silhouette_cluster_2 = [silhouette_scores[i] for i in range(len(assigned_clusters)) if assigned_clusters[i] == 1]\n",
    "print(\"Cluster C2 Silhouette Score:\", sum(silhouette_cluster_2)/len(silhouette_cluster_2) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b7336c",
   "metadata": {},
   "source": [
    "### 4) [0.5v] Knowing the purity of the clustering solution is 0.75, identify the number of possible classes (ground truth)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826b5357",
   "metadata": {},
   "source": [
    "Feita em papel (nÃ£o necessita de grandes cÃ¡lculos)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
